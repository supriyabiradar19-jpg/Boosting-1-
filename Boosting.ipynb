{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsG6yABEjX7i"
      },
      "outputs": [],
      "source": [
        "1.  What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "\n",
        "    -> Boosting is an iterative, sequential ensemble learning technique that combines multiple \"weak learners\"\n",
        "       to build a single, highly accurate \"strong learner\" by focusing on the mistakes of previous models.\n",
        "\n",
        "\n",
        "      #  How Boosting Improves Weak Learners\n",
        "\n",
        "# 1. Sequential Training:\n",
        "            Unlike methods that train models independently (like bagging), boosting trains weak learners one after another in a specific order.\n",
        "# 2. Weighting Misclassified Data:\n",
        "                After each weak learner is trained, the algorithm identifies the data points that were misclassified.\n",
        "                   It then assigns higher weights to these difficult-to-classify examples for the next iteration.\n",
        "# 3. Error Correction:\n",
        "             The subsequent weak learner is trained to focus on these higher-weighted, previously misclassified instances,\n",
        "                effectively trying to \"correct\" the errors of its predecessors.\n",
        "# 4. Iterative Improvement:\n",
        "            This process of adjusting weights and retraining continues iteratively. Each new model adds to the overall predictive\n",
        "               power of the ensemble by focusing on the remaining mistakes.\n",
        "# 5. Final Combination:\n",
        "        The predictions of all the weak learners are combined, often with different weights, to form a final, strong predictive model that is\n",
        "    significantly more accurate and robust than any individual weak learner."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2.  : What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "\n",
        "      ->  AdaBoost and Gradient Boosting both train models sequentially, but AdaBoost increases the weights of misclassified\n",
        "           data points in subsequent iterations to focus on them\n",
        "\n",
        "\n",
        "          #  AdaBoost Training Method\n",
        "\n",
        "# Weighted Data Points:\n",
        "             AdaBoost starts by assigning equal weights to all training data points.\n",
        "# Iterative Training:\n",
        "            It then trains a series of weak learners (models).\n",
        "# Increase Weights:\n",
        "           After each model is trained, it identifies the misclassified data points and increases their weights, making them more important for the next model.\n",
        "# Weighted Combination:\n",
        "              The final prediction is formed by a weighted combination of all weak learners, with models that performed better having more influence.\n",
        "\n",
        "# Gradient Boosting Training Method\n",
        "\n",
        "1. Sequential Models:\n",
        "          Gradient Boosting also trains models sequentially, but the focus is on minimizing errors.\n",
        "2. Error Prediction:\n",
        "           Each new model is trained to predict the residuals, or the error, made by the previous model.\n",
        "3. Gradient of Loss Function:\n",
        "        To do this, it calculates the gradient of the loss function (a measure of error) to determine the optimal direction to reduce the error in each step.\n",
        "# 4. Iterative Improvement:\n",
        "The process iteratively adds new models that are specifically trained to correct the errors left by the previous ones, progressively reducing the overall loss.\n",
        "Key Difference Summarized\n",
        "# AdaBoost:\n",
        "Focuses on re-weighting data points that were misclassified by previous models"
      ],
      "metadata": {
        "id": "0w45NjmWkEzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3.  How does regularization help in XGBoost?\n",
        "\n",
        "      # ->  XGBoost incorporates several regularization techniques\n",
        "      :\n",
        "# L1 (Lasso) and L2 (Ridge) Regularization (alpha and lambda parameters):\n",
        "\n",
        "These add penalty terms to the objective function based on the magnitude of the leaf weights.\n",
        "L1 regularization (alpha):\n",
        "             adds a penalty proportional to the absolute value of the leaf weights.\n",
        "       This can lead to sparsity by driving less important leaf weights to zero, effectively performing feature selection at the leaf level.\n",
        "# L2 regularization (lambda):\n",
        "                  adds a penalty proportional to the squared magnitude of the leaf weights. This encourages smaller, more distributed weights,\n",
        "                 preventing any single leaf from having too much influence and promoting a more stable model.\n",
        "\n",
        "\n",
        "# Tree Complexity Control (gamma parameter):\n",
        "Gamma:\n",
        "             is a minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
        "       *  A higher gamma value makes the algorithm more conservative, preventing the growth of complex trees that might\n",
        "           overfit the training data. If the gain from a split is less than gamma, the split is not performed, effectively pruning the tree.\n",
        "# Subsampling:\n",
        "The subsample parameter controls the fraction of training instances randomly sampled for building each tree.\n",
        "Training on a subset of the data for each tree introduces randomness and helps to reduce variance, thereby mitigating overfitting\n",
        "\n"
      ],
      "metadata": {
        "id": "f27C26QTkE08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. : Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "    # ->  Key reasons for CatBoost's efficiency in handling categorical data:\n",
        "\n",
        "# Ordered Target Encoding:\n",
        "                CatBoost employs a specialized form of target encoding that addresses the issue of target leakage, a common problem in\n",
        "               traditional target encoding methods. It does this by using a permutation-driven approach where the target mean for a category is\n",
        "                 calculated based on a subset of the data that does not include the current sample, thus preventing information from the target variable\n",
        "                  from \"leaking\" into the feature encoding.\n",
        "# Native Handling of Categorical Features:\n",
        "Unlike many other gradient boosting algorithms that require explicit one-hot encoding or other manual transformations of categorical features,\n",
        " CatBoost can directly process them. It automatically identifies and converts categorical features into numerical representations using its internal\n",
        "  algorithms, saving significant preprocessing time and effort.\n",
        "# Ordered Boosting:\n",
        "            CatBoost introduces a novel \"ordered boosting\" scheme that helps to prevent prediction shift, a phenomenon where the distribution of\n",
        "            gradients changes during training, potentially leading to overfitting. By using a specific ordering of data points during tree construction\n",
        "             and gradient calculation, CatBoost ensures that the gradients are calculated based on unbiased estimates, leading to more robust models.\n",
        "# Symmetric Trees:\n",
        "CatBoost utilizes symmetric trees, also known as oblivious trees, where the same splitting condition is applied at each level of the tree.\n",
        " This structure allows for more efficient computation and parallelization, further contributing to faster training times, especially wit\n",
        " h large datasets and numerous categorical features.\n",
        "  *  These features combined allow CatBoost to effectively leverage the information contained within categorical variables while maintaining\n",
        " high performance and reducing the risk of overfitting, making it a highly efficient choice for datasets with a significant presence of categorical data.\n"
      ],
      "metadata": {
        "id": "JymQGjDIkE4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5.  : What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "\n",
        "      #  -> Specific Applications for Boosting:\n",
        "Fraud Detection:\n",
        "         Boosting models like Gradient Boosting can focus on and correct misclassified transactions by assigning higher weights to them in subsequent\n",
        "           training rounds, leading to more precise detection of fraudulent activities.\n",
        "# Credit Scoring:\n",
        "          The iterative learning in boosting allows models to become highly precise in assessing credit risk by continuously refining their predictions\n",
        "            based on past data and focusing on individuals likely to default.\n",
        "# Customer Churn Prediction:\n",
        "               Boosting's ability to reduce bias and improve predictive power makes it suitable for identifying customers likely to leave a service, as it\n",
        "                 can sequentially learn from complex patterns in the data.\n",
        "# Image and Object Identification:\n",
        "              Boosting algorithms can achieve high performance in image and object recognition tasks by progressively improving the identification of features and objects.\n",
        "# Data Mining and Sentiment Analysis:\n",
        "                The focus on reducing bias in boosting makes it ideal for applications where a strong predictive model is needed to uncover subtle\n",
        "                   patterns and sentiment in large datasets.\n"
      ],
      "metadata": {
        "id": "fj0E-OMBkE55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6.  Write a Python program to:\n",
        " ● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize AdaBoost Classifier\n",
        "model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50Wx1ilZkE9g",
        "outputId": "1ccbbce9-c95b-46ef-dff2-48d6e143ed39"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "7.\n",
        "Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1,\n",
        "                                  max_depth=3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate with R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg60OB9Omr8R",
        "outputId": "b5b769e2-bfc8-4d5a-aa80-24ad437815fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared Score: 0.8004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "8.  Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        " ● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define parameter grid for learning_rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZEZHnnjnF7b",
        "outputId": "28e61d7f-58a5-4f00-f8ff-2abf33dbf820"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Test Accuracy: 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [16:58:43] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "6c4bdc98",
        "outputId": "fc025835-00f2-4283-8b46-fc0b2e40df8c"
      },
      "source": [
        "9.   Write a Python program to:\n",
        " ● Train a CatBoost Classifier\n",
        " ● Plot the confusion matrix using seaborn\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 1. Data Preparation (Example using a dummy dataset)\n",
        "# In a real-world scenario, load your dataset here (e.g., pd.read_csv('your_data.csv'))\n",
        "data = {\n",
        "    'feature1': np.random.rand(100),\n",
        "    'feature2': np.random.rand(100) * 10,\n",
        "    'feature3': np.random.randint(0, 2, 100), # Categorical feature\n",
        "    'target': np.random.randint(0, 2, 100) # Binary target variable\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "X = df[['feature1', 'feature2', 'feature3']]\n",
        "y = df['target']\n",
        "\n",
        "# Identify categorical features\n",
        "categorical_features_indices = [X.columns.get_loc('feature3')]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Train a CatBoost Classifier\n",
        "model = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    loss_function='Logloss',\n",
        "    random_seed=42,\n",
        "    verbose=False  # Suppress training output for cleaner console\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train, cat_features=categorical_features_indices)\n",
        "\n",
        "# 3. Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 4. Calculate and Plot the Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for CatBoost Classifier')\n",
        "plt.show()\n",
        "\n",
        "# Optional: Print accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "10. : You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -->   1. Data Preprocessing & Handling Missing/Categorical Values\n",
        "\n",
        "Load and Inspect Data:\n",
        "             Load the dataset and use functions like info() and describe() to understand its structure, data types, and summary statistics.\n",
        "Handle Missing Values:\n",
        "Imputation:\n",
        "             For numerical features, use mean, median, or a model-based imputer. For categorical features, use the mode or a constant value.\n",
        "Consideration:\n",
        "               Given the imbalanced nature, you may want to impute missing values in a way that doesn't further skew the data.\n",
        "# Handle Categorical Features:\n",
        "Encoding:\n",
        "      Since boosting models can handle categorical features directly, consider using techniques like one-hot encoding or,\n",
        "       preferably, CatBoost's built-in categorical encoding to avoid high dimensionality and potential data leakage.\n",
        "Handle Imbalanced Data:\n",
        "Resampling:\n",
        "          Address the class imbalance by oversampling the minority class (defaulted loans) using techniques like SMOTE (Synthetic Minority\n",
        "          Over-sampling Technique) or undersampling the majority class.\n",
        "Class Weights:\n",
        "         Alternatively, some boosting algorithms (like XGBoost and CatBoost) have built-in parameters to assign class weights to give more importance\n",
        "           to the minority class during training.\n",
        "Feature Scaling:\n",
        "             While not strictly necessary for tree-based models, it can be beneficial for some boosting algorithms or when comparing with other\n",
        "                models, and it is a good practice for overall robustness.\n",
        "\n",
        "# 2. Choice Between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "CatBoost:\n",
        "            Recommended for this use case because it excels at handling categorical features directly and automatically, which simplifies\n",
        "                the preprocessing step significantly and improves performance.\n",
        "XGBoost:\n",
        "          A very popular and efficient gradient boosting library, but it requires one-hot encoding for categorical features, which can increase\n",
        "             the feature space and potentially lead to issues if not managed well.\n",
        "AdaBoost:\n",
        "           An older, foundational algorithm. While effective, it can be sensitive to noisy data and may not be as performant or efficient\n",
        "                as XGBoost or CatBoost for complex datasets with many features.\n",
        "\n",
        "# 3. Hyperparameter Tuning Strategy\n",
        "\n",
        "Grid Search CV:\n",
        "    A systematic approach where you define a grid of hyperparameter values and evaluate all combinations using cross-validation.\n",
        "  This is good but can be computationally expensive.\n",
        "# Bayesian Optimization:\n",
        "          A  more advanced and efficient method that uses probabilistic models to predict the performance of hyperparameters and\n",
        "             intelligently chooses the next set of parameters to evaluate. This can find optimal values faster than grid search.\n",
        "\n",
        "# Key Hyperparameters to Tune (for XGBoost/CatBoost):\n",
        "\n",
        "n_estimators (number of boosting rounds)\n",
        "learning_rate (step size shrinkage)\n",
        "max_depth (maximum depth of trees)\n",
        "subsample and colsample_bytree (data and feature subsampling)\n",
        "reg_alpha and reg_lambda (L1 and L2 regularization)\n",
        "scale_pos_weight (for handling imbalanced datasets in XGBoost)\n",
        "\n",
        "# 4. Evaluation Metrics\n",
        "\n",
        "Precision:\n",
        "Of all the loans predicted as defaults, what percentage actually defaulted. High precision minimizes false positives (flagging a good loan as a default).\n",
        "Recall (Sensitivity):\n",
        "Of all the actual defaults, what percentage did the model identify. High recall minimizes false negatives (failing to identify a defaulting loan).\n",
        "F1-Score:\n",
        "The harmonic mean of precision and recall, providing a single metric that balances both.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DLM57TYZoWhX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}